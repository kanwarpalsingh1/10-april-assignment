{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89af274a-3e2c-428b-9eef-d9198e39a324",
   "metadata": {},
   "source": [
    "Q1. To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can apply Bayes' theorem. Let:\n",
    "\n",
    "A be the event that an employee is a smoker.\n",
    "B be the event that an employee uses the health insurance plan.\n",
    "We are given:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "0.70\n",
    "P(B)=0.70 (probability of using the health insurance plan)\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.40\n",
    "P(A∣B)=0.40 (probability of being a smoker given that the employee uses the health insurance plan)\n",
    "Using Bayes' theorem:\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "×\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)×P(A)\n",
    "​\n",
    " \n",
    "\n",
    "Substituting the given values:\n",
    "0.40\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "×\n",
    "1\n",
    "0.70\n",
    "0.40= \n",
    "0.70\n",
    "P(B∣A)×1\n",
    "​\n",
    " \n",
    "\n",
    "Solving for \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∣A):\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.40\n",
    "×\n",
    "0.70\n",
    "1\n",
    "=\n",
    "0.28\n",
    "P(B∣A)=0.40× \n",
    "1\n",
    "0.70\n",
    "​\n",
    " =0.28\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.28 or 28%.\n",
    "\n",
    "Q2. The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed to handle:\n",
    "\n",
    "Bernoulli Naive Bayes is suitable for binary feature vectors where each feature represents a binary value (e.g., presence or absence of a term in a document).\n",
    "Multinomial Naive Bayes is suitable for discrete feature vectors where each feature represents the frequency count of occurrences of different categories (e.g., word counts in a document).\n",
    "Q3. Bernoulli Naive Bayes handles missing values by considering them as a separate category. When a feature value is missing for a sample, it is treated as if the feature is not present in the sample. Therefore, when training the Bernoulli Naive Bayes model, missing values are implicitly handled without requiring imputation.\n",
    "\n",
    "Q4. Yes, Gaussian Naive Bayes can be used for multi-class classification. In Gaussian Naive Bayes, each class is assumed to follow a Gaussian (normal) distribution, and the model calculates the likelihood of a sample belonging to each class based on its feature values. Therefore, it can be applied to problems with more than two classes, making it suitable for multi-class classification tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83618891-1fc2-44e0-be98-14a9aa761c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
